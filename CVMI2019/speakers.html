<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <meta content="width=device-width,initial-scale=1" name="viewport">
  <meta content="description" name="description">
  <meta name="google" content="notranslate" />
  <meta content="Mashup templates have been developped by Orson.io team" name="author">

  <!-- Disable tap highlight on IE -->
  <meta name="msapplication-tap-highlight" content="no">

  <link href="./assets/apple-icon-180x180.png" rel="apple-touch-icon">
  <link href="./assets/favicon.ico" rel="icon">



  <title>CVMI</title>

<link href="./main.82cfd66e.css" rel="stylesheet"></head>

<body>

 <!-- Add your content of header -->
<header class="">
  <div class="navbar navbar-default visible-xs">
    <button type="button" class="navbar-toggle collapsed">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a href="./index.html" class="navbar-brand"><strong>CVMI</strong></a>
  </div>

  <nav class="sidebar">
    <div class="navbar-collapse" id="navbar-collapse">
      <div class="site-header hidden-xs">
          <!--<h3><strong>CVMI</strong></h3>-->
          <img class="img-responsive" alt="" src="./assets/images/img-001.png" height:200 width:200>
      </div>
      <style type="text/css">
        ul{
          list-style: none;
          padding:0;
          overflow:hidden;
          display:table;
          height:auto;
          margin:0 auto;
        }
        ul li{
          float: left;
        }
        ul a{
          color: #fff;
          width: 200px;
          padding: 10px;
          background: #bebebe;
          text-decoration: none;
          text-align: center;
          font-weight: bold;
          display: block;
        }
        ul li ul{
          display: none;
        }
        ul li ul li{
          float: none;
          list-style: none;
          margin-top: 2px;
        }
        ul li ul li a:hover{
          background: #06f;
        }
        ul li:hover ul{
          display: block;
        }
        body {
          color: black;
          background-color:floralwhite}
      </style>
      <ul class="nav">
         <li><a href="./new.html" title=""><I><font size="4" color="red"><strong>New</strong></font></I></a></li>
        <li><a href="./index.html" title=""><font size="4"><strong>Welcome</strong></font></a></li>
        <li><a href="./callForPaper.html" title=""><font size="4"><strong>Call for Papers</strong></font></a></li>
        <li><a href="./organizers.html" title=""><font size="4"><strong>Organizers</strong></font></a></li>
        <li><a href="./comittee.html" title=""><font size="4"><strong>Program Committee</strong></font></a></li>
        <li><a href="./speakers.html" title=""><font size="4"><strong>Invited Speakers</strong></font></a></li>
        <li><a href="./dates.html" title=""><font size="4"><strong>Important Dates</strong></font></a></li>
        <li><a href="./submission.html" title=""><font size="4"><strong>Submission</strong></font></a></li>
         <li><a href="./accepted.html" title=""><font size="4"><strong>Accepted Papers</strong></font></a></li>
        <li><a href="./program.html" title=""><font size="4"><strong>Program</strong></font></a></li>
        <li><a href="./venue.html" title=""><font size="4"><strong>Venue</strong></font></a></li>
        <li><a href="./contact.html" title=""><font size="4"><strong>Contact</strong></font></a></li>
        <li><a href="./pastcvmi.html" title=""><font size="4"><strong>Past CVMIs</strong></font></a></li>
      </ul>
    </div>
  </nav>
</header>
<main class="" id="main-collapse">


  <div class="hero-full-wrapper">
    <div class="grid">
    <div class="gutter-sizer"></div>
      <div class="grid-sizer"></div>

        <img class="img-responsive" alt="" src="./assets/images/img-0001.png">
        <style>
        .grid img{
           -moz-box-shadow:6px 6px 7px #000;
           -webkit-box-shadow:6px 6px 7px #000;
           box-shadow:6px 6px 7px #000;
           filter:progid:DXImageTransform.Microsoft.Shadow(Strength=6,Direction=135,Color='#000000');
        }
        </style>
        <br />
        <h2 id="welcome">Invited Speakers</h2>
<hr style="height:1px;border:none;border-top:1px solid #555555;" /> 

      <font size="4">
        <p>  <strong>Title:</strong>  The ImageJ Ecosystem: An Open and Extensible Platform for Biomedical Image Analysis </p>
        <p>  <strong> Start Time:</strong>  9:00 AM   </p>
          <p> <strong>Speaker:</strong> Professor Kevin Eliceiri, Director, Laboratory for Optical and Computational Instrumentation, University of Wisconsin–Madison </p>
          
        <p><center>
  <img class="img-responsive" alt="" src="./assets/images/eliceiri_kevin.jpg" width="200" height="500" >
  </center></p>
       
          <p>
           <strong> Abstract:</strong> Biological imaging has greatly advanced over the last thirty years with the now unprecedented ability to track biological phenomena in high resolution in physiologically relevant conditions over time and in space. As these imaging technologies mature and become main stream tools for the bench biologist there is great need for improved software tools that drive the informatics workflow of the imaging process from acquisition and image analysis to
           visualization and dissemination. To best meet the workflow challenges, these tools need to be freely available, open source, and transparent in their development and deployment. In particular it is clear that given the complexity, and heterogeneity of the modern image dataset, there cannot be a single software solution. Different imaging processing and visualization approaches need access not only to the data but also to each other. There needs to be compatibility not only in file import and export but interoperability in preserving and communicating what was done to the image. There is a great opportunity in achieving this interoperability, tools that can talk to each other not only enable new biological discovery but also efficiencies in sharing code and in many cases more precise workflows. We present our efforts towards interoperability and extensibility in the ImageJ consortium including partners such as the Cell Profiler, FIJI, KNIME, and Open Microscopy Environment groups. We are actively developing key software libraries like Bio-Formats, ImgLib and ImageJ Ops that are utilized to analyze and visualize biological image data, to the developmental benefit of not only of the applications but the libraries themselves. 
           
            </p>
        
   <p>  <strong>Bio: </strong> 
    Dr. Kevin Eliceiri is the Walter H. Helmerich Research Chair and Associate Professor of Medical Physics and Biomedical Engineering at the University of Wisconsin at Madison. He is an Investigator in the Morgridge Institute for Research and member of the Carbone Cancer Center and McPherson Eye Research Institute.  He is director of the Laboratory for Optical and Computational Instrumentation, a biophotonics laboratory dedicated to the development and application of optical and computational technologies for cell studies.  The Eliceiri lab is a lead developer in several open source imaging packages including FIJI and ImageJ. His instrumentation efforts involve novel forms of polarization, laser scanning and multiscale imaging.  Dr. Eliceiri has authored more than 180 scientific papers on various aspects of optical imaging, image analysis, cancer and live cell imaging. <font size="4">
     </p>  
     
   
  <hr style="height:1px;border:none;border-top:1px solid #555555;" /> 

        <p>  <strong>Title:</strong> Phenotypic Screening of HiPSC Derived Neurons: Balancing Throughput with Relevance</p>
        <p>  <strong> Start Time:</strong>  9:30 AM   </p>
        <p> <strong>Speaker:</strong> Dr. Anne Bang, Director, Cell Biology, Conrad Prebys Center for Chemical Genomics at Sanford Burnham Medical Research Institute </p>
        
        <p><center>
<img class="img-responsive" alt="" src="./assets/images/Anne-Bang.jpg"  width="300" height="700">
</center></p>
                
        <p>
         <strong> Abstract:</strong> 
         Patient specific induced pluripotent stem cells (iPSC) complement traditional cell-based assays used in drug discovery and could aid in the development of clinically useful compounds. They are scalable, allow interrogation of differentiated features of human cells not reflected by immortalized lines, and importantly, carry disease-specific traits in complex genetic backgrounds that can impact disease phenotypes. Development of technology platforms to perform compound screens of iPSC with relatively high-throughput will be essential to realize their potential for disease modeling and drug discovery. Towards this goal, we have been working to develop assay platforms to interrogate fundamental aspects of neuronal morphology and physiology, providing a basis for further development of more complex phenotypic readouts and compound screens based on patient specific hiPSC-derived neurons. We will discuss our screening results and development of patient cell specific and hiPSC based models for testing of drugs on disease relevant cell types.
        </p>   
                  
<p> <strong>Bio: </strong> Dr. Anne Bang joined the Sanford Burnham Prebys Medical Discovery Institute in June 2010 as Director of Cell Biology at the Conrad Prebys Center for Chemical Genomics, a state-of-the-art academic drug discovery center.  Her current research efforts are directed at developing patient-specific, induced pluripotent stem cell (iPSC)-based disease models of neurological disease that reflect higher order cellular functions and recapitulate disease phenotypes yet have the throughput and robustness necessary for drug discovery. Her goal is to use these models for target ID and drug screening to develop clinically useful compounds. Prior to joining SBP she served as Director of Stem Cell Research at ViaCyte Inc, where her efforts focused on process optimization and advancing Viacyte’s cell therapy product into development, scaled manufacturing, product characterization, and safety assessment. Dr. Bang received a B.S. from Stanford University, a Ph.D. in Biology from UCSD, and was a post-doctoral fellow at the Salk Institute. 
 </p> 
 <!---  
 <p>
  Dr. Bang was previously at ViaCyte Inc. (formerly Novocell Inc.), since 2005, where as Director of Stem Cell Research she oversaw a group of scientists working to develop human embryonic stem cells as a replenishable source of pancreatic cells for the treatment of diabetes. She received her Ph.D. in Biology from the University of California in San Diego, and was a post-doctoral fellow and a Senior Research Scientist in the Neurobiology Laboratory at The Salk Institute for Biological Studies, in La Jolla, CA.   
</p>
-->     
   
<hr style="height:1px;border:none;border-top:1px solid #555555;" / > 

<p>  <strong>Title:</strong> Case Study on Applying Deep Learning Methods to High Content Image-Based Assays </p>
    <p>  <strong> Start Time:</strong>  10:00 AM   </p>
 <p> <strong>Speaker:</strong> Subhashini Venugopalan, Google Accelerated Science Team, Google Research  </p>
 <p>
<p><center>
    <img class="img-responsive" alt="" src="./assets/images/subha_face_med.jpg" width="250" height="600">
</center></p>
  <strong> Abstract:</strong> 
  In this study we investigate whether high-content imaging of primary skin fibroblasts stained with Cell Painting could reveal disease-relevant information across subjects. First, we use a pre-trained deep neural network and analysis with deep image embeddings to show that technical features such as batch/plate-type, plate and location within a plate lead to detectable nuisance signals. Using a plate design and image acquisition strategy that accounts for these variables, we performed a pilot study with 12 healthy control and 12 subjects affected by Spinal Muscular Atrophy (SMA) and used a convolutional neural network to evaluate whether a model trained on cells from a subset of the 24 subjects could distinguish disease state on cells from the remaining unseen subjects.
 </p>     
<p>  <strong>Bio: </strong> 
  Subhashini Venugopalan is a research scientist with Google applying machine learning to medical images and audio. She received her PhD from University of Texas at Austin working on problems in the intersection of computer vision, deep learning, and natural language processing. She was a recipient of the University of Texas Dissertation Fellowship. As a reviewer, she has served on the program committee of several conferences (CVPR, AAAI, NAACL, ACL, EMNLP) and journals (IJCV, TPAMI), and has received an outstanding reviewer award (EMNLP 2018). Subhashini received her master's degree from Indian Institute of Technology (IIT) Madras, and a bachelor's degree from National Institute of Technology (NIT) Karnataka, India.<font size="4">
</p>   
<hr style="height:1px;border:none;border-top:1px solid #555555;" />
<p>  <strong>Title:</strong> What Concussions Do to Brain Cells: A Deep Look</p>
<p>  <strong> Start Time:</strong>  13:00 PM   </p>
<p> <strong>Speaker:</strong> Professor Badri Roysam, Chair of Electrical and Computer Engineering, University of Houston </p> 

<p><center>
<img class="img-responsive" alt="" src="./assets/images/roysam.jpg" width="250" height="600">
</center></p>
<p>
  <strong> Abstract:</strong> 
  At the cellular level, traumatic brain injury (TBI) initiates a complex web of pathological alterations in all the types of brain cells, ranging from individual cells to multi-cellular functional units at multiple scales ranging from niches to the layered brain cytoarchitecture. Unfortunately, current immunohistochemistry (IHC) methods reveal only a fraction of these alterations at a time, miss the many other alterations and side effects that are occurring concurrently, and do not provide quantitative readouts. The potential consequence of unobserved and untreated cellular alterations is high, as they may contribute to confounding, co-morbid, or persistent conditions (e.g., depression, headaches, stress-related health problems). Importantly, the current state of drug development for brain pathologies leaves much to be desired, with a recent review concluding that “most of the pharmacologic and non-pharmacologic treatments have failed to demonstrate significant efficacy on both the clinical symptoms as well as the pathophysiologic cascade responsible for the permanent brain injury”. In this talk, I will describe a practical approach to pathological brain tissue mapping with a focus on combination drug treatment. Our approach is based on replacing the many low information content assays with a single comprehensive assay based on imaging and analyzing highly multiplexed whole brain sections using 10 – 50 molecular markers, sufficient to analyze all the major brain cell types and their functional states over extended regions.
 </p>         
<p>  <strong>Bio: </strong> 
  Badri Roysam (Fellow IEEE, AIMBE) is the Hugh Roy and Lillie Cranz Cullen University Professor, and Chairman of the Electrical and Computer Engineering Department at the University of Houston (2010 – present). From 1989 to 2010, he was a Professor at Rensselaer Polytechnic Institute in Troy, New York, USA, where he directed the Rensselaer unit of the NSF Engineering Research (ERC) Center for Subsurface Sensing and Imaging Systems (CenSSIS ERC), and co-directed the Rensselaer Center for Open Source Software (RCOS) that was funded by a major alumnus gift. He received the Doctor of Science degree from Washington University, St. Louis, USA, in 1989. Earlier, he received his Bachelor’s degree in Electronics from the Indian Institute of Technology, Madras, India in 1984.
Badri’s research is on the applications of multi-dimensional signal processing, machine learning, big-data bioinformatics, high-performance computing to problems in fundamental and clinical biomedicine. He collaborates with a diverse group of biologists, physicians, and imaging researchers. His work focuses on automated analysis of 2D/3D/4D/5D microscopy images from diverse applications including cancer immunotherapy, traumatic brain injury, retinal diseases, neural implants, learning and memory impairments, binge alcohol, tumor mapping, stem-cell biology, stroke research, and neurodegenerative diseases.<font size="4">
</p>     
        
            
 <hr style="height:1px;border:none;border-top:1px solid #555555;" />    

      <p>  <strong>Title:</strong> Accelerating Drug Discovery Through the Power of Microscopy Images </p>
      <p>  <strong> Start Time:</strong>  13:30 PM   </p>
      <p> <strong>Speaker:</strong> Allen Goodman, senior software engineer at Broad institute of MIT and Harvard </p>
  
<p><center>
<img class="img-responsive" alt="" src="./assets/images/Allen Goodman.png" width="250" height="650">
</center></p>
<p>
  <strong> Abstract:</strong> 
  An overview of a user-friendly deep learning-based application in collaboration with the Horvath laboratory. This browser-based phenotype classifier uses deep learning and is yet to be named. It will replace the Carpenter lab's prior CellProfiler Analyst and the Horvath lab's Advanced Cell Classifier tools and is currently in prototype stage.
 </p> 
<p>  <strong>Bio: </strong> 
Allen Goodman is a gifted software engineer, recognized for writing pioneering web and mobile applications. He was a founding engineer and Director of Research of Simple, a highly successful customer-oriented online bank startup, where he co-wrote their critically acclaimed mobile and web applications. Then, as a senior software engineer for Chef—the leading software automation company—he co-wrote Chef Analytics, a real-time message broker for cloud computing. His technical expertise includes cross-platform proficiency, fluency in general-purpose and scientific programming languages, and expert knowledge of popular tools, methodologies, and best practices. Goodman decided to apply his software engineering talents to biomedicine and joined the Carpenter laboratory in August 2015, where he has dramatically reshaped CellProfiler, and is leading the group’s efforts in developing new bioimage analysis tools based on deep learning methods. He was also recently named an Imaging Software Fellow, an award by the Chan Zuckerberg Initiative to support funding of open-source software efforts to improve image analysis and visualization in biomedicine.
  <font size="4">
</p>  

<hr style="height:1px;border:none;border-top:1px solid #555555;" />

<!--
    <p>  <strong>Title:</strong> CNNs Detect very Early Stem Cell Differentiation in A Simple Microscopic Image </p>
    <p>  <strong> Start Time:</strong>  16:20 PM   </p>
    <p> <strong>Speaker:</strong> Dr. Santiago Miriuka  </p>
<p><center>
<img class="img-responsive" alt="" src="./assets/images/Santiago Miriuka.jpg" width="250" height="650">
</center></p>
<p>
  <strong> Abstract:</strong> 
  Stem cells differentiate into any type of adult cells. Detecting this differentiation can be costly and time-consuming. Therefore, we trained a CNN with transmitted light microscopy images to distinguish pluripotent stem cells from early differentiating cells. We induced differentiation of mouse embryonic stem cells to epiblast-like cells and took images at several time points from the initial stimulus. We found that the networks can be trained to recognize undifferentiated cells from differentiating cells with an accuracy higher than 99%. Successful prediction started just 20 min after the onset of differentiation. Furthermore, CNNs displayed great performance in several similar pluripotent stem cell (PSC) settings. Accurate cellular morphology recognition in a simple microscopic set up may have a significant impact on how cell assays are performed in the near future.
 </p>     
<p>  <strong>Bio: </strong> 
  Dr. Miriuka is a Clinical and Basic Investigator at FLENI-CONICET, and a Clinical Cardiologist specialized in Heart Failure and Heart Transplantation at the Department of Cardiology, Fundación FLENI, Buenos Aires, Argentina. His lab is interested on pluripotent stem cell and its differentiation into heart tissue, working on identifying the RNA landscape of human heart development. Dr Miriuka has published more than 50 scientific papers in biomedicine. His group is interested on applying deep learning to categorize and identify the early features of stem cell differentiation, either through imaging or transcriptomic analyzes. Dr. Miriuka’s lab has recently published the use of deep learning to identify very early signs of differentiation on pluripotent stem cells. As soon as the cells start to differentiate, there are minimal signs that can be identified by neural networks on a simple microscopic image. This detection is fast, accurate and easy, providing a useful alternative to classical analyses of stem cell differentiation. Potentially, these findings can paved the way to automatic working setups in cell biology. The dynamic landscape of stem cell differentiation offers many opportunities to apply artificial intelligence to speed up findings that lead to its successful application in biomedicine.<font size="4">
</p>   
             
<hr style="height:1px;border:none;border-top:1px solid #555555;" />
-->

<p>  <strong>Title:</strong> Interoperable Web Computational Plugins for Large Microscopy Image Analyses </p>
<p>  <strong> Start Time:</strong>  16:20 PM   </p>
<p> <strong>Speaker:</strong> Peter Bajcsy, NIST and Nathan Hotaling, NIH </p>
<table align="center" cellpadding="100px"><tr>
    <td><img class="img-responsive" alt="" src="./assets/images/Peter Bajcsy.jpg" width="250" height="300" border=10><center><strong>Peter Bajcsy</strong></center></td>
    <td> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</td>
    <td><img class="img-responsive" alt="" src="./assets/images/Nathan Hotaling.jpg" width="180" height="600" border=10><center><strong>Nathan Hotaling</strong></center></td>
</tr></table>
<strong> Abstract:</strong> 
There is an increasing interest in enabling discoveries from high-throughput and high content microscopy imaging of biological specimens and material structures under a variety of conditions. As multi-dimensional automated imaging increases its throughput to thousands of images per hour, the computational infrastructure for handling the images has become a major bottleneck. The bottleneck associated challenges arise due to big image data, complex phenomena to model, non-trivial computational scalability that accommodates advanced hardware and cutting-edge algorithms, and incompatible software tools that vary in the language they were written in, platform they were written for, and capabilities they were designed to execute.
</p> 
<p>
To address the above challenges, groups have developed solutions that leverage modern web technologies on the client side and a spectrum of databases, computational workflow engines, and communication protocols on the server side to hide the infrastructure complexity. However, these solutions have not focused on inter-operability, specifically as it relates to domain specific computational plugins.
</p>
<p>
To address the inter-operability of computational web plugins and to develop an open source platform for executing web-based image processing pipelines over very large image collections, the National Institute of Standards and Technology (NIST) along with the National Institutes of Health (NIH) - National Center for Advancing Translational Science (NCATS) have formed a close collaboration. The plugins developed by both institutes are based on software containers as standardized units for deployment, as well as on dynamically created web user interfaces (UI) to enter parameters needed for the software execution. Each container packages code, with all its dependencies, and has an entry point for running the computation in any computing environment. Each UI description file contains metadata about the plugin container and the computation parameters.  
</p>
<p>
We will demonstrate the utility of the pipeline system with web plugins by analyzing large multi-channel fluorescent images of whole murine eyes to assess increased accumulation of auto- fluorescent waste products in c57bl/6 mice with gene ABCA4 selectively knocked-out (KO) to quantitatively assess disease state and progression in both KO and control models. With the NIST and NIH NCATS combined efforts, researchers are enabled to discover quantitative insights from their imaging data and reuse computational tools developed by anyone following the web computational plugin conventions.
</p>     
<p>  <strong>Bio: </strong> 
Dr. Peter Bajcsy received his Ph.D. in Electrical and Computer Engineering in 1997 from the University of Illinois at Urbana-Champaign (UIUC) and a M.S. in Electrical and Computer Engineering in 1994 from the University of Pennsylvania (UPENN).  He worked for machine vision, government contracting, and research and educational institutions before joining the National Institute of Standards and Technology (NIST) in 2011. At NIST, he has been leading a project focusing on the application of computational science in biological metrology, and specifically stem cell characterization at very large scales. Peter’s area of research is large-scale image-based analyses and syntheses using mathematical, statistical and computational models while leveraging computer science foundations for image processing, machine learning, computer vision, and pattern recognition. He has co-authored more than more than 32 journal papers and eight books or book chapters, and close to 100 conference papers. <font size="4">
</p> 
<p>
Nathan Hotaling is a Lead Data Scientist within the Information Resources Technology Branch at NCATS where he is responsible for overseeing and developing the next generation of artificially intelligent image analysis tools. He received his PhD in Biomedical Engineering and a masters in clinical research in 2013 from the Georgia Institute of Technology and Emory University. After his PhD, Nathan did post-doctoral research in a joint project between the National Institute of Standards and Technology (NIST) and the National Eye Institute (NEI) where he developed a platform to use in an Investigational New Drug application to the FDA for a therapy of Age-related Macular Degeneration (AMD) using induced pluripotent stem cells derived from patients with AMD.  While pursuing this project he began to develop a platform to analyze high content image data-sets collected for drug screening and cell bio-manufacturing. This work led to his transition to his current position where he oversees the development of a scalable image analysis platform to non-invasively assess cell and tissue architecture, functionality, phenotype, consistency, and viability.  Using this platform with novel machine learning and deep learning techniques he intends to unlock the next “omics” of cell analysis, Vis-omics, for both research and clinical projects.
</p>  
<hr style="height:1px;border:none;border-top:1px solid #555555;" />
 <!--       
<p><center>
<img class="img-responsive" alt="" src="./assets/images/Michelle Freund.png" width="200" height="650">
</center></p>
      
<p>Dr. Michelle Freund is Scientific Program Coordinator in the Office of Technology Development and Coordination at the National Institute of Mental Health, NIH, Bethesda, MD. As a program officer, she manages a research portfolio of grants that are focused on the development of novel tools and technologies important for the advancement of basic and translational neuroscience. 
  Michelle serves as the Director for the NIH NeuroBioBank, a network of six brain and tissue repositories that provide post-mortem human brain samples for research. She is an active member of several trans-NIH interdisciplinary teams such as the NIH BRAIN Initiative and the Blueprint for Neuroscience. As co-lead on a BRAIN Initiative team, she provides guidance and oversight for the Cells and Circuits focus area outlined in the BRAIN 2025 report. Michelle received a B.A. from the University of California, San Diego in mammalian physiology and a Ph.D. in Neuroscience from Hahnemann University in Philadelphia. 
  Before joining NIH in 2007, she studied the role of monoamine neurotransmitters in the actions of antidepressant drugs and the interactions of stress and drug addiction.<font size="4">
  </p>   

  <hr>
      
   <p><center>
<img class="img-responsive" alt="" src="./assets/images/Daniel Hoeppner.png"  width="200" height="500">
</center></p>
      
    <p>  Dr. Daniel Hoeppner is an Associate Director of Neuroscience at Astellas Research Institute of America (ARIA), in San Diego CA. 
      He manages a diverse team of scientists focused on identifying new therapeutic interventions for patients suffering with psychiatric illness. 
      His team applies the tools of human genetics, next-generation sequencing, human iPSC (stem cell) technology, mouse models, and automated microscopy to identify and validate novel targets for intervention.  
      Prior to Astellas, Dr. Hoeppner was an investigator at the Lieber Institute for Brain Development (Johns Hopkins, Baltimore MD), a Staff Scientist at the NIH-NINDS (Bethesda MD), and a graduate student of genetics at Cold Spring Harbor Laboratory (Long Island, NY).  

    </p> 


 <hr>  
<p><center>
<img class="img-responsive" alt="" src="./assets/images/jieyang.jpg"  width="200" height="500">
</center></p>
      
    <p>  Dr. Jie Yang is a Program Director in the Division of Information and Intelligent Systems (IIS) at the National Science Foundation (NSF).
      He is the leader of the Robust Intelligence Cluster and oversees computer vision research. He has also been involved in many other crosscutting programs. Before joining NSF, he was a faculty member in the School of Computer Science at Carnegie Mellon University, where he worked on multimodal/multimedia, computer vision, pattern recognition, robotics, and automatic control. 
      He was an Advisory Board member of the ACM International Conference on Multimodal Interfaces (2003-2015). He also served as Program Co-chair (2002) and General Co-chair (2006 and 2010) for ACM International Conference on Multimodal Interfaces, as well as a Program Co-chair (2010) for ACM Multimedia. 
      He served as a General Co-chair (2014) for the IEEE International Conference on Multimedia & Expo. He served as an Associate Editor for the IEEE Transaction on Multimedia (2004-2008).  He is an Associate Editor for the Machine Vision Applications journal.
      He also served as Area Chair and Program Committee member for multimodal/multimedia, computer vision, and pattern recognition conferences. He is a fellow of IEEE.<font size="4">
      
    </p> 
  
  

   
   
   
   
   
   
   
   
   
   
      
      <hr> 
       <p>  <strong>Title:</strong>  Assessment of intra-tumor heterogeneity and evolutionary trajectories in cancer </p>
      <p>  <strong> Start Time:</strong>  13:30 PM   </p>
        <p> <strong>Speaker:</strong> Dr. Subhajyoti De, Rutgers, the State University of New Jersey and Rutgers Cancer Institute </p>
        <p><center>
        <img class="img-responsive" alt="" src="./assets/images/desubho2012.jpg" width="200" height="500" >
</center></p> 
        
        
        <p> <strong> Abstract:</strong> 
           Nuclei segmentation plays an important role in digital pathology image analysis as the accurate separation of nuclei is crucial for cancer diagnosis and other clinical analysis. Current learning-based methods in nuclei segmentation have certain limitations in lowering localization accuracy. We develop FullNet, a full resolution convolutional neural network that achieves good performance in nuclei segmentation. Using annotated histopathological images, we show the effectiveness of this approach in nuclei segmentation and compare it to other state-of-the-art methods. We then describe our initiatives to integrate histopathological imaging and genomic data to inform about intra-tumor heterogeneity and evolutionary trajectories in cancer. Analyzing histopathological images for 55 BRCA1-associated breast tumors, we assess the status of BRCA1, PTEN, and p53 mutations at the single cell level. We then use computational methods to predict the relative temporal order of somatic events, on the basis of the frequency of cells with single or combined alterations. Although there is no obligatory order of events, we find that loss of PTEN is the most common first event and is associated with basal-like subtype, whereas in the majority of luminal tumors, mutation of TP53 occurs first and mutant PIK3CA is rarely detected. We also observe intratumor heterogeneity for the loss of wild-type BRCA1 and increased cell proliferation and centrosome amplification in the normal breast epithelium of BRCA1 mutation carriers. Our results have important implications for the design of chemopreventive and therapeutic interventions in this high-risk patient population.
          </p> 
        <p>  <strong>Bio: </strong> 
          Dr. Subhajyoti De is an Assistant Professor of Pathology and Laboratory Medicine at Rutgers, the State University of New Jersey and a member of Rutgers Cancer Institute. Previously, he completed his PhD at the University of Cambridge, UK and postdoctoral work at Harvard University. His research group develops and applies genomics, systems biology, and computational modeling approaches to cancer. 
         </p> 
        
      <hr> 
        
       <p>  <strong>Title:</strong> Image Analysis Methods for Measuring the Micro-Environment of Single Cells Within Three-Dimensional Multicellular Spheroids: Optimization for Breakthroughs in Drug Discovery.</p> 
        <p>  <strong> Start Time:</strong>  14:00 PM   </p> 
        <p> <strong>Speaker:</strong> Dr. Ty Voss, Voss Imaging Informatics (LLC)</p>
                    <p><center>
<img class="img-responsive" alt="" src="./assets/images/Ty Voss.jpg" width="200" height="500" >
</center></p>  
        
        <p> <strong> Abstract:</strong> 
         The natural in vivo three-dimensional (3D) organization of complex tissues allows individual cells and collections of localized cell-types to signal and behave in ways that may not be possible when those cells are grown in mono-layer tissue culture. These 3D micro-environments are important for understanding diverse disease states including cancer, chronic wound healing, auto-immune diseases, and neuro-degenerative syndromes. Accordingly, the pharmaceutical research industry has recently committed significant resources towards measuring the behavior of multicellular spheroids/organoids in more high-throughput modalities for screening purposes. However, the majority of these published high-throughput imaging studies have only measured ‘whole spheroid-level responses’ and have lacked single-cell level information. In contrast, several academic studies have shown that single-cell level 3D imaging measurements can be obtained from organoid samples, but the image acquisition and analysis methods do not have sufficient throughput for screening purposes. In this presentation, we will describe our recent efforts to optimize sample preparation, high-throughput image acquisition, and parallelized automated 3D image analysis. This convergence of optimized methods provides single-cell level data from large numbers of spheroid samples, which will be essential for ongoing drug discovery screens. 
        </p>
        
        <p> <strong>Bio: </strong> 
          Ty C. Voss received a BSc degree in biochemistry from Oklahoma State University in 1994 and then studied for his molecular biology doctoral degree at Tulane University. During this period, he began quantifying and computationally modeling the effects of hormonal regulation on transcription at the single cell level. He extended these studies and developed more advanced automated microscopy approaches during two fellowships, first at the University of Virginia and then at the NIH Campus in Bethesda MD. In 2009, He began collaborating with NCI Sr. Research Faculty to establish the NCI High-Throughput Imaging Facility (HiTIF). He served as the HiTIF head for four years, developing and executing automated microscopy assays, along with siRNA screening campaigns. In 2013, He founded a bio-informatics business, Voss Imaging Informatics (LLC), that provides consulting services and customized analytical software solutions for multiple government, academic, and commercial research clients. These contracted consulting projects are presently ongoing (2018). Dr. Voss also acted as Senior Applications Scientist for North America with the PerkinElmer Cellular Imaging and Analysis Team during 2014-2016.
       </p> 
        
        
        <hr>
        
      <p>  <strong>Title:</strong>   Life and death decisions- classification, characterization and predictions of death in neuronal models of neurodegenerative disease</p>
       <p>  <strong> Start Time:</strong>  16:30 PM   </p>
        <p> <strong>Speaker:</strong> Dr. Jeremy Linsley, Gladstone Institutes and the University of California, San Francisco </p>
            <p><center>
<img class="img-responsive" alt="" src="./assets/images/JeremyLinsley.jpeg" width="200" height="500" >
</center></p>  
        
         <p> <strong> Abstract:</strong> 
           Understanding the cellular changes that destine a neuron to live or to die is a fundamental challenge to understanding neurodegenerative disease. We use robotic microscopy, an imaging platform that uses automated identification and tracking of millions of individual neurons in culture over time, to quantitatively relate intermediate changes within a neuron to its fate. Interpreting changes within degenerating neurons is complicated both by the task of analyzing dense and complex microscopy images, as well as by the complication of analyzing progressive and often sporadic diseases. Using convolutional and recurrent neural networks, we quantify, interpret, and predict features in human patient derived neurons. These in silico methods reliably predict standard conventional cell-biological techniques, facilitate measurements that would be problematic or impossible to acquire using conventional methods, and outperform human manual curation. By surveying neurons derived from patients across neurodegenerative disease, we are developing algorithms for classifying unhealthy and diseased neurons across a variety of neurodegenerative diseases. Additionally, we are able to reliably predict the life or death of a neuron prior to death, focusing our window for understanding the mechanisms for these progressive diseases. We believe the use of these technologies will enhance our understanding of the pathology and physiology underlying neurodegenerative diseases with the goal of creating new therapies and cures for these devastating diseases.
         </p>
         <p>  <strong>Bio: </strong> 
        Dr. Jeremy Linsley is a postdoctoral fellow in Steve Finkbeiner’s lab in the Center for Systems and Therapeutics at the Gladstone Institutes and the University of California, San Francisco. Jeremy studies neurodegenerative disease models using four-dimensional imaging, novel fluorescent biosensors, and deep learning analysis. He combines artificial intelligence approaches with a fully automated robotic microscope developed by the Finkbeiner laboratory that can track individual cells for hours, days, or even months. Using these tools, he is developing cutting-edge technologies to detect, predict, and analyze a range neurodegenerative diseases. By shedding light on underlying pathology and physiology, Linsley’s work aims to create new therapies and cures for these devastating diseases. Linsley earned a PhD in molecular and cellular biology from the University of Michigan, where he studied a family of adaptor-like proteins. He explained the mechanism of action for one of the proteins in skeletal muscle and discovered that a mutation in one of the proteins was the basis for the debilitating Native American myopathy disease.
         </p> 
        
        
    <hr>
                <p>  <strong>Title:</strong> Efficient and Scalable Tools for Large Scale Microscopy Image Analysis</p> 
         <p>  <strong> Start Time:</strong>  17:00 PM   </p>
         <p> <strong>Speaker:</strong> Dr. Erhan Bas, Janelia Research Campus of HHMI </p>
                    <p><center>
<img class="img-responsive" alt="" src="./assets/images/Erhan Bas.png" width="200" height="500" >
</center></p>  
        
        <p> <strong> Abstract:</strong> With the advances in hardware and computational tools, large-scale high resolution volumetric imaging of whole systems is possible. Analysis of large volumes requires scalable segmentation algorithms and visualization platforms to efficiently stream through data and utilize user feedback. We developed an interactive segmentation and visualization framework for proofreading of large (tens of TBs) sparse volumetric datasets that aims to minimize user interaction with directed segmentation workflows. 
          Developed tools enabled the analysis and proofread of more than 500TBs of volumetric data and resulted in the largest axonal reconstruction database available(<a href="https://http://ml-neuronbrowser.janelia.org/">https:http://ml-neuronbrowser.janelia.org/</a>).
          </p>
            
        <p> <strong>Bio: </strong> 
          Erhan Bas received his PhD in Electrical and Computer Engineering from Northeastern University, Boston in 2011. He joined the Mouselight team at Janelia Research Campus (JRC) of HHMI as a computer scientist in 2015. At JRC, he is interested in large scale image analysis techniques for neuronal morphology analysis. Before joining to JRC, he was with the computer vision lab at GE Global Research in Niskayuna, NY, where he led a team of material and computer scientists working on industrial inspection technologies and he was an adjunct professor at Rensselaer Polytechnic University (RPI), Troy, NY where he taught Biological Image Analysis course. His general research interests include machine vision and statistical pattern recognition with various applications in biomedical and industrial image processing. He is one of the five Diadem challenge finalists and he serves as a member of IEEE Bio Imaging and Signal Processing Technical Committee, where he has been serving as an organizing committee member and area chair of ICIP 2015 and ISBI 2015&2018.
         </p> 
    <hr>

        -->
      
    </div>

</main>

<script>
document.addEventListener("DOMContentLoaded", function (event) {
  navbarToggleSidebar();
  navActivePage();
});
</script>

<!-- Google Analytics: change UA-XXXXX-X to be your site's ID

<script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
  ga('create', 'UA-XXXXX-X', 'auto');
  ga('send', 'pageview');
</script>

--> <script type="text/javascript" src="./main.85741bff.js"></script></body>

</html>
